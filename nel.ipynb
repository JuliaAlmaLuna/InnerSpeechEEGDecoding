{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dataLoader as dl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "#from Inner_Speech_Dataset.Plotting.ERPs import \n",
    "from Inner_Speech_Dataset.Python_Processing.Data_extractions import  Extract_data_from_subject\n",
    "from Inner_Speech_Dataset.Python_Processing.Data_processing import  Select_time_window, Transform_for_classificator, Split_trial_in_time\n",
    "from Inner_Speech_Dataset.Python_Processing.Data_processing import  Calculate_power_windowed\n",
    "from Inner_Speech_Dataset.Python_Processing.Utilitys import picks_from_channels\n",
    "from Inner_Speech_Dataset.Python_Processing.Data_processing import Average_in_frec\n",
    "#Frequencies\n",
    "\n",
    "from scipy.fft import rfft, ifft, fftshift, fftfreq\n",
    "\n",
    "\n",
    "#Separate into equal 5 buckets\n",
    "def sepFreqIndexBuckets(freqs2, nr_of_buckets = 5): \n",
    "     \n",
    "    bucket_size_amp = np.sum(freqs2)/nr_of_buckets\n",
    "    #print(bucket_size_amp)\n",
    "    \n",
    "    buckets = np.zeros([nr_of_buckets, 2])\n",
    "    bucket = []\n",
    "    cur_buck_size = 0\n",
    "    \n",
    "    b = 0\n",
    "    c = 0\n",
    "    for index, freqs in enumerate(freqs2,0):\n",
    "        cur_buck_size += freqs\n",
    "        bucket.append(index)\n",
    "        if cur_buck_size > bucket_size_amp:\n",
    "            buckets[b] = [0 + c , c + len(bucket)]\n",
    "            b += 1\n",
    "            c += len(bucket)\n",
    "            #print(len(bucket))\n",
    "            bucket = []\n",
    "            cur_buck_size = 0\n",
    "            \n",
    "    \n",
    "    buckets[b] = [0 + c , c + len(bucket)]\n",
    "    #print(len(bucket)) \n",
    "       \n",
    "    return buckets  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def createFreqBuckets(data, nr_of_buckets = 5):\n",
    "\n",
    "\n",
    "    nr_of_buckets = 5\n",
    "    buckets = np.zeros([nr_of_buckets, 2])\n",
    "    for trial in data:\n",
    "        for channel in trial:\n",
    "            buckets += sepFreqIndexBuckets(abs(rfft(channel))[:(channel.shape[0]//2)]*1000, nr_of_buckets)\n",
    "            \n",
    "    buckets = buckets/(data.shape[0]*data.shape[1])\n",
    "    \n",
    "    \n",
    "    return np.int32(buckets)\n",
    "\n",
    "\n",
    "def data_into_freq_buckets(data, nr_of_buckets, buckets):\n",
    "\n",
    "    freqAmps = np.zeros([data.shape[0], data.shape[1], nr_of_buckets])\n",
    "    for tr_nr, trial in enumerate(data):\n",
    "        for ch_nr, channel in enumerate(trial):\n",
    "            for b in range(nr_of_buckets):\n",
    "                ff_c = abs(rfft(channel))*1000\n",
    "                freqAmps[tr_nr, ch_nr, b] = np.sum(ff_c[int(buckets[b, 0]):int(buckets[b,1])])\n",
    "    return freqAmps\n",
    "\n",
    "#Channel name array\n",
    "\n",
    "def arrToDict(arr):\n",
    "    dict = {}\n",
    "    for row in arr:\n",
    "        dict[row[0]] = row[1]\n",
    "    \n",
    "    return dict\n",
    "\n",
    "def get_channelNames():\n",
    "    ch_names = np.array(dl.get_channelnames())\n",
    "    nr = np.arange(ch_names.shape[0])\n",
    "    ch_names = np.array([ch_names, nr]).T\n",
    "    ch_names = arrToDict(ch_names)\n",
    "    return ch_names\n",
    "\n",
    "def only_spec_channel_data(data , picks):\n",
    "    \n",
    "    channel_names_string = picks_from_channels(picks)\n",
    "    ch_names = get_channelNames()\n",
    "    channel_nr = []\n",
    "    for name in  channel_names_string:\n",
    "        channel_nr.append(int(ch_names[name]))\n",
    "        #print(ch_names[name])\n",
    "\n",
    "    channel_nr = np.array(channel_nr)\n",
    "    \n",
    "    #print(channel_nr)\n",
    "    #data = np.swapaxes(data, 0, 1)\n",
    "    #labels = np.swapaxes(labels, 0, 1)\n",
    "    #for channelnrs in channels:\n",
    "    data2 = np.delete(data, np.delete(np.arange(128), channel_nr) , axis=1)\n",
    "    return data2\n",
    "\n",
    "\n",
    "def get_power_array(split_data , samplingRate, trialSplit = 1, t_min = 0, t_max = 0.99):\n",
    "\n",
    "    #trialSplit = 16\n",
    "    sR = samplingRate #samplingRate = 32\n",
    "    data_power = np.zeros([split_data.shape[0], split_data.shape[1], trialSplit, 2])\n",
    "    for t, trial in enumerate(split_data,0):\n",
    "        for c, channel in enumerate(trial,0):\n",
    "            for x in range(trialSplit):\n",
    "                data_power[t, c, x, : ] = Calculate_power_windowed(channel, fc=sR, window_len=1/8, window_step=1/8, t_min=t_min*(1/trialSplit), t_max=t_max*(1/trialSplit))\n",
    "\n",
    "\n",
    "    #m_power , std_power\n",
    "    #print(data_power.shape)\n",
    "    return data_power\n",
    "    \n",
    "\n",
    "\n",
    "#Loading the data and labels from EEG and EXG\n",
    "\n",
    "# data1, labels1 = dl.load_data(datatype=\"EEG\", subject_nr=1, verbose=True,sampling_rate=sampling_rate) \n",
    "# data2, labels2 = dl.load_data(datatype=\"EEG\", subject_nr=2 ,verbose=True,sampling_rate=sampling_rate )\n",
    "# data4 , labels4 = dl.load_data(datatype=\"EEG\", subject_nr=4, verbose=True,sampling_rate=sampling_rate) \n",
    "\n",
    "# dataX, labelsX = dl.load_data(datatype=\"EXG\", verbose=False) \n",
    "# #datab, labelsb = dl.load_data(datatype=\"baseline\", verbose=False, sampling_rate=32) \n",
    "# #dl.load_data(datatype2=2) #4.5 is max\n",
    "\n",
    "####\n",
    "\n",
    "#data = np.concatenate([data1, data2, data4], axis = 0)\n",
    "#labels1d = np.concatenate([labels1, labels2, labels4], axis = 0)\n",
    "\n",
    "####\n",
    "\n",
    "#data = data1\n",
    "#labels1d = labels1\n",
    "\n",
    "sampling_rate = 128\n",
    "data, labels = dl.load_multiple_datasets(nr_of_datasets=8, sampling_rate=sampling_rate, t_min=2, t_max=3)\n",
    "ch_names = get_channelNames()\n",
    "\n",
    "# print(data.shape)\n",
    "# dataCL = only_spec_channel_data(data, \"CL\")\n",
    "# dataCZ = only_spec_channel_data(data, \"CZ\")\n",
    "# dataPZ = only_spec_channel_data(data, \"PZ\")\n",
    "# dataOPZ = only_spec_channel_data(data, \"OPZ\")\n",
    "\n",
    "# def avg_channels(data):\n",
    "#     avg_data = np.mean(data, axis=1)\n",
    "#     print(avg_data.shape)\n",
    "#     return np.reshape(avg_data, [avg_data.shape[0], 1 , avg_data.shape[1]])\n",
    "\n",
    "# data = np.concatenate([avg_channels(dataCL), avg_channels(dataCZ),\n",
    "#                 avg_channels(dataPZ), avg_channels(dataOPZ),], axis=1)\n",
    "# print(data.shape)\n",
    "\n",
    "#Not work when time is not 4.5 right now kinda\n",
    "print(data.shape)\n",
    "data_p =  get_power_array(data, sampling_rate, trialSplit=1).squeeze()\n",
    "print(data_p.shape)\n",
    "#print(data_p[:,:,1])\n",
    "\n",
    "#Getting Freq Data \n",
    "nr_of_buckets = 5\n",
    "buckets = createFreqBuckets(data, nr_of_buckets)\n",
    "print(\"buckets\")\n",
    "print(buckets)\n",
    "data_f = data_into_freq_buckets(data, nr_of_buckets, buckets)\n",
    "print(data_f.shape)\n",
    "\n",
    "\n",
    "## Normalize data\n",
    "\n",
    "\n",
    "print(data_f.shape)\n",
    "print(data_p.shape)\n",
    "print(labels.shape)\n",
    "print(data_p[2,5])\n",
    "data = np.concatenate([data_f, data_p, data], axis =2 )\n",
    "print(data.shape)\n",
    "data = keras.utils.normalize(data, axis=1, order=2)\n",
    "print(data.shape)\n",
    "#print(data[2,5])\n",
    "#print(labels[2])\n",
    "print(labels[:].shape)\n",
    "# def get_local_frequency(split_data , samplingRate, trialSplit = 2,):\n",
    "\n",
    "#     #trialSplit = 16\n",
    "#     sR = samplingRate #samplingRate = 32\n",
    "#     data_power = np.zeros([split_data.shape[0], split_data.shape[1], trialSplit, 2])\n",
    "#     for t, trial in enumerate(split_data,0):\n",
    "#         for c, channel in enumerate(trial,0):\n",
    "#             for x in range(trialSplit):\n",
    "#                 data_power[t, c, x, : ] = Calculate_power_windowed(channel, fc=sR, window_len=1*4/trialSplit, window_step=1*4/trialSplit, t_min=0, t_max=4/trialSplit * x + 4/trialSplit)\n",
    "    \n",
    "\n",
    "#     #m_power , std_power\n",
    "#     #print(data_power.shape)\n",
    "#     return data_power\n",
    "    \n",
    "#Splitting into training and test data\n",
    "#print(labels)\n",
    "\n",
    "\n",
    "order = np.arange(labels.shape[0])\n",
    "np.random.shuffle(order)\n",
    "\n",
    "temp_data = np.zeros(data.shape)\n",
    "temp_labels = np.zeros(labels.shape)\n",
    "\n",
    "for x in range(labels.shape[0]):\n",
    "    i = order[x]\n",
    "    \n",
    "    temp_data[x] = data[i]\n",
    "    temp_labels[x] = labels[i]\n",
    "\n",
    "data = temp_data\n",
    "labels = temp_labels\n",
    "\n",
    "data_train, data_test = np.split(data, indices_or_sections=[int(labels.shape[0]*0.75)],axis=0)\n",
    "labels_train, labels_test = np.split(labels, indices_or_sections=[int(labels.shape[0]*0.75)],axis=0)\n",
    "print(labels_train.shape)\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "#Convolution part\n",
    "\n",
    "# Do the convs like I wrote in the Note\n",
    "# First need to find out 8 best channels\n",
    "#  Utilities can help average similar channels\n",
    "# Read through article and how they did it better!\n",
    "# convlayer = tf.keras.layers.Conv2D(4,\n",
    "#  kernel_size=[2,1], input_shape= (data_train.shape[0], data_train.shape[1] , data_train.shape[2], 1,), \n",
    "#  padding=\"valid\", strides=[round(data_train.shape[1]),1],\n",
    "#  activation=\"relu\")\n",
    "\n",
    "\n",
    "# print(data_train.shape)\n",
    "# print(data_test.shape)\n",
    "\n",
    "# #data_train = np.swapaxes(data_train, 2, 1)\n",
    "# #data_test = np.swapaxes(data_test, 2, 1)\n",
    "# print(data_train.shape)\n",
    "# print(data_test.shape)\n",
    "# data_train = np.expand_dims(data_train, axis=0)\n",
    "# data_test = np.expand_dims(data_test, axis=0)\n",
    "# data_train = np.moveaxis(data_train, 0, -1)\n",
    "# data_test = np.moveaxis(data_test, 0, -1)\n",
    "\n",
    "# conv_data_train = convlayer(data_train)\n",
    "# conv_data_test = convlayer(data_test)\n",
    "\n",
    "# data_train = conv_data_train\n",
    "# data_test = conv_data_test\n",
    "\n",
    "# #data_train = np.swapaxes(data_train, 2, 1)\n",
    "# #data_test = np.swapaxes(data_test, 2, 1)\n",
    "\n",
    "\n",
    "# data_train = np.squeeze(np.moveaxis(data_train, -1, 0))\n",
    "# data_test = np.squeeze(np.moveaxis(data_test, -1, 0))\n",
    "# print(data_train.shape)\n",
    "# print(data_test.shape)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "eeg_model = tf.keras.Sequential([\n",
    "    \n",
    "    #layers.Conv2D(500,\n",
    "    #kernel_size=[1,8], input_shape= (1, data_train.shape[1], data_train.shape[2], 1,), \n",
    "    #padding=\"valid\", strides=[1,4],\n",
    "    #activation=\"relu\"),\n",
    "    # layers.Conv2D(200,\n",
    "    # kernel_size=[1,32], \n",
    "    # padding=\"valid\", strides=1,\n",
    "    # activation=\"relu\"),\n",
    "    # layers.Conv2D(200,\n",
    "    # kernel_size=[1,32], \n",
    "    # padding=\"valid\", strides=1,\n",
    "    # activation=\"relu\"),\n",
    "    # layers.Conv2D(200,\n",
    "    # kernel_size=[1,32], \n",
    "    # padding=\"valid\", strides=4,\n",
    "    # activation=\"relu\"),\n",
    "    layers.LocallyConnected1D(128, input_shape = (data_train.shape[1],data_train.shape[2]), \n",
    "     kernel_size=50, \n",
    "     padding=\"valid\", strides=1,\n",
    "     activation=\"relu\"),\n",
    "    layers.Flatten(input_shape = (data_train.shape[1],data_train.shape[2])),\n",
    "    #layers.Dense(units=28*3, activation=\"relu\"),\n",
    "    #layers.Dropout(0.3),\n",
    "    #layers.Dense(units=28*28, activation=\"relu\"),\n",
    "    #layers.Dropout(0.3),\n",
    "    layers.Dense(units=28*28, activation=\"relu\"),\n",
    "    #layers.Dense(units=20, activation=\"relu\"),\n",
    "    #layers.Dense(units=5, activation=\"relu\"),\n",
    "    #layers.Dense(units=1, activation=\"relu\"),\n",
    "    #layers.Flatten(),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(units=28*28, activation=\"relu\"),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(units=28, activation=\"relu\"),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(units=2, activation=\"softmax\")\n",
    "\n",
    "\n",
    "])\n",
    "eeg_model.build()\n",
    "eeg_model.summary()\n",
    "\n",
    "\n",
    "for trialNr, trial in enumerate(data_train[43:47],1):\n",
    "    for channel in trial[3:4]:\n",
    "        plt.figure()\n",
    "        plt.plot(channel)\n",
    "        plt.title(\"EEG {}\".format(labels_train[trialNr]))\n",
    "\n",
    "print(labels[2])\n",
    "print(labels[:].shape)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "callback2 = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=10, restore_best_weights=True)\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(histogram_freq=2)\n",
    "\n",
    "eeg_model.compile(optimizer='adam',\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "data_train_send = np.reshape(data_train, [data_train.shape[0], 1 , data_train.shape[1], data_train.shape[2], 1])\n",
    "#data_train_send = np.reshape(data_train, [data_train.shape[0], 1 , data_train.shape[1], data_train.shape[2], 1])\n",
    "\n",
    "data_test_send = np.reshape(data_test, [data_test.shape[0], 1 , data_test.shape[1], data_test.shape[2], 1])\n",
    "#data_test_send = np.reshape(data_test, [data_test.shape[0], 1 , data_test.shape[1], data_test.shape[2], 1])\n",
    "\n",
    "outputs = eeg_model.fit(data_train, labels_train, callbacks=[callback, callback2], epochs=20) #\n",
    "\n",
    "print(\"Results\")\n",
    "eeg_model.evaluate(data_test, labels_test)\n",
    "result = eeg_model.predict(data_test)\n",
    "\n",
    "result2 = []\n",
    "print(result)\n",
    "for res in result:\n",
    "    x = np.maximum(res[0], res[1])\n",
    "    result2.append(int(np.where(res == x)[0]))\n",
    "\n",
    "result2 = np.array(result2)\n",
    "print(result2)\n",
    "\n",
    "\n",
    "\n",
    "for trialNr, trial in enumerate(data[44:46],44):\n",
    "    for channel in trial[5:6]:\n",
    "        plt.figure()\n",
    "        plt.plot(channel)\n",
    "        plt.title(\"EEG {}\".format(labels[trialNr]))\n",
    "\n",
    "      \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
