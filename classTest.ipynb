{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dataLoader as dl\n",
    "\n",
    "\n",
    "#from Inner_Speech_Dataset.Plotting.ERPs import \n",
    "from Inner_Speech_Dataset.Python_Processing.Data_extractions import  Extract_data_from_subject\n",
    "from Inner_Speech_Dataset.Python_Processing.Data_processing import  Select_time_window, Transform_for_classificator, Split_trial_in_time\n",
    "from Inner_Speech_Dataset.Python_Processing.Data_processing import  Calculate_power_windowed\n",
    "from Inner_Speech_Dataset.Python_Processing.Utilitys import picks_from_channels\n",
    "from Inner_Speech_Dataset.Python_Processing.Data_processing import Average_in_frec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequencies\n",
    "\n",
    "from scipy.fft import rfft, ifft, fftshift, fftfreq\n",
    "\n",
    "\n",
    "#Separate into equal 5 buckets\n",
    "def sepFreqIndexBuckets(freqs2, nr_of_buckets = 5): \n",
    "     \n",
    "    bucket_size_amp = np.sum(freqs2)/nr_of_buckets\n",
    "    #print(bucket_size_amp)\n",
    "    \n",
    "    buckets = np.zeros([nr_of_buckets, 2])\n",
    "    bucket = []\n",
    "    cur_buck_size = 0\n",
    "    \n",
    "    b = 0\n",
    "    c = 0\n",
    "    for index, freqs in enumerate(freqs2,0):\n",
    "        cur_buck_size += freqs\n",
    "        bucket.append(index)\n",
    "        if cur_buck_size > bucket_size_amp:\n",
    "            buckets[b] = [0 + c , c + len(bucket)]\n",
    "            b += 1\n",
    "            c += len(bucket)\n",
    "            #print(len(bucket))\n",
    "            bucket = []\n",
    "            cur_buck_size = 0\n",
    "            \n",
    "    \n",
    "    buckets[b] = [0 + c , c + len(bucket)]\n",
    "    #print(len(bucket)) \n",
    "       \n",
    "    return buckets  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def createFreqBuckets(data, nr_of_buckets = 5):\n",
    "\n",
    "\n",
    "    nr_of_buckets = 5\n",
    "    buckets = np.zeros([nr_of_buckets, 2])\n",
    "    for trial in data:\n",
    "        for channel in trial:\n",
    "            buckets += sepFreqIndexBuckets(abs(rfft(channel))[:(channel.shape[0]//2)]*1000, nr_of_buckets)\n",
    "            \n",
    "    buckets = buckets/(data.shape[0]*data.shape[1])\n",
    "    \n",
    "    \n",
    "    return np.int32(buckets)\n",
    "\n",
    "\n",
    "def data_into_freq_buckets(data, nr_of_buckets, buckets):\n",
    "\n",
    "    freqAmps = np.zeros([data.shape[0], data.shape[1], nr_of_buckets])\n",
    "    for tr_nr, trial in enumerate(data):\n",
    "        for ch_nr, channel in enumerate(trial):\n",
    "            for b in range(nr_of_buckets):\n",
    "                ff_c = abs(rfft(channel))*1000\n",
    "                freqAmps[tr_nr, ch_nr, b] = np.sum(ff_c[int(buckets[b, 0]):int(buckets[b,1])])\n",
    "    return freqAmps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: [trials x channels x samples]\n",
      "(500, 128, 128)\n",
      "Labels shape\n",
      "(500, 4)\n",
      "Final data shape\n",
      "(100, 128, 128)\n",
      "Final labels shape\n",
      "(100,)\n",
      "Up is 0.0 and Down is 1.0\n",
      "Up is 0.0 and Down is 1.0\n",
      "Up is 0.0 and Down is 1.0\n",
      "Up is 0.0 and Down is 1.0\n",
      "Up is 0.0 and Down is 1.0\n",
      "Up is 0.0 and Down is 1.0\n",
      "Up is 0.0 and Down is 1.0\n",
      "(788, 128, 128)\n",
      "(788, 128)\n",
      "(788, 128)\n",
      "(788, 128)\n",
      "(788, 128)\n",
      "(788, 4, 128)\n",
      "(788, 4, 2)\n",
      "buckets\n",
      "[[ 0  3]\n",
      " [ 3  8]\n",
      " [ 8 19]\n",
      " [19 40]\n",
      " [40 63]]\n",
      "(788, 4, 5)\n"
     ]
    }
   ],
   "source": [
    "#Channel name array\n",
    "\n",
    "def arrToDict(arr):\n",
    "    dict = {}\n",
    "    for row in arr:\n",
    "        dict[row[0]] = row[1]\n",
    "    \n",
    "    return dict\n",
    "\n",
    "def get_channelNames():\n",
    "    ch_names = np.array(dl.get_channelnames())\n",
    "    nr = np.arange(ch_names.shape[0])\n",
    "    ch_names = np.array([ch_names, nr]).T\n",
    "    ch_names = arrToDict(ch_names)\n",
    "    return ch_names\n",
    "\n",
    "def only_spec_channel_data(data , picks):\n",
    "    \n",
    "    channel_names_string = picks_from_channels(picks)\n",
    "    ch_names = get_channelNames()\n",
    "    channel_nr = []\n",
    "    for name in  channel_names_string:\n",
    "        channel_nr.append(int(ch_names[name]))\n",
    "        #print(ch_names[name])\n",
    "\n",
    "    channel_nr = np.array(channel_nr)\n",
    "    \n",
    "    #print(channel_nr)\n",
    "    #data = np.swapaxes(data, 0, 1)\n",
    "    #labels = np.swapaxes(labels, 0, 1)\n",
    "    #for channelnrs in channels:\n",
    "    data2 = np.delete(data, np.delete(np.arange(128), channel_nr) , axis=1)\n",
    "    return data2\n",
    "\n",
    "\n",
    "def get_power_array(split_data , samplingRate, trialSplit = 1, t_min = 0, t_max = 1):\n",
    "\n",
    "    #trialSplit = 16\n",
    "    sR = samplingRate #samplingRate = 32\n",
    "    data_power = np.zeros([split_data.shape[0], split_data.shape[1], trialSplit, 2])\n",
    "    for t, trial in enumerate(split_data,0):\n",
    "        for c, channel in enumerate(trial,0):\n",
    "            for x in range(trialSplit):\n",
    "                data_power[t, c, x, : ] = Calculate_power_windowed(channel, fc=sR, window_len=sR/8, window_step=sR/8, t_min=t_min*(1/trialSplit), t_max=t_max*(1/trialSplit))\n",
    "    \n",
    "\n",
    "    #m_power , std_power\n",
    "    #print(data_power.shape)\n",
    "    return data_power\n",
    "    \n",
    "\n",
    "\n",
    "#Loading the data and labels from EEG and EXG\n",
    "\n",
    "# data1, labels1 = dl.load_data(datatype=\"EEG\", subject_nr=1, verbose=True,sampling_rate=sampling_rate) \n",
    "# data2, labels2 = dl.load_data(datatype=\"EEG\", subject_nr=2 ,verbose=True,sampling_rate=sampling_rate )\n",
    "# data4 , labels4 = dl.load_data(datatype=\"EEG\", subject_nr=4, verbose=True,sampling_rate=sampling_rate) \n",
    "\n",
    "# dataX, labelsX = dl.load_data(datatype=\"EXG\", verbose=False) \n",
    "# #datab, labelsb = dl.load_data(datatype=\"baseline\", verbose=False, sampling_rate=32) \n",
    "# #dl.load_data(datatype2=2) #4.5 is max\n",
    "\n",
    "####\n",
    "\n",
    "#data = np.concatenate([data1, data2, data4], axis = 0)\n",
    "#labels1d = np.concatenate([labels1, labels2, labels4], axis = 0)\n",
    "\n",
    "####\n",
    "\n",
    "#data = data1\n",
    "#labels1d = labels1\n",
    "\n",
    "sampling_rate = 128\n",
    "data, labels = dl.load_multiple_datasets(nr_of_datasets=8, sampling_rate=sampling_rate, t_min=2, t_max=3)\n",
    "ch_names = get_channelNames()\n",
    "\n",
    "print(data.shape)\n",
    "dataCL = only_spec_channel_data(data, \"CL\")\n",
    "dataCZ = only_spec_channel_data(data, \"CZ\")\n",
    "dataPZ = only_spec_channel_data(data, \"PZ\")\n",
    "dataOPZ = only_spec_channel_data(data, \"OPZ\")\n",
    "\n",
    "def avg_channels(data):\n",
    "    avg_data = np.mean(data, axis=1)\n",
    "    print(avg_data.shape)\n",
    "    return np.reshape(avg_data, [avg_data.shape[0], 1 , avg_data.shape[1]])\n",
    "\n",
    "data = np.concatenate([avg_channels(dataCL), avg_channels(dataCZ),\n",
    "                avg_channels(dataPZ), avg_channels(dataOPZ),], axis=1)\n",
    "print(data.shape)\n",
    "\n",
    "#Not work when time is not 4.5 right now kinda\n",
    "data_p =  get_power_array(data, sampling_rate, trialSplit=1).squeeze()\n",
    "print(data_p.shape)\n",
    "\n",
    "#Getting Freq Data \n",
    "nr_of_buckets = 5\n",
    "buckets = createFreqBuckets(data, nr_of_buckets)\n",
    "print(\"buckets\")\n",
    "print(buckets)\n",
    "data_f = data_into_freq_buckets(data, nr_of_buckets, buckets)\n",
    "print(data_f.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(788, 4, 5)\n",
      "(788, 4, 2)\n",
      "(788, 2)\n",
      "(788, 4, 7)\n"
     ]
    }
   ],
   "source": [
    "print(data_f.shape)\n",
    "print(data_p.shape)\n",
    "print(labels.shape)\n",
    "data = np.concatenate([data_f, data_p], axis =2 )\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_local_frequency(split_data , samplingRate, trialSplit = 2,):\n",
    "\n",
    "#     #trialSplit = 16\n",
    "#     sR = samplingRate #samplingRate = 32\n",
    "#     data_power = np.zeros([split_data.shape[0], split_data.shape[1], trialSplit, 2])\n",
    "#     for t, trial in enumerate(split_data,0):\n",
    "#         for c, channel in enumerate(trial,0):\n",
    "#             for x in range(trialSplit):\n",
    "#                 data_power[t, c, x, : ] = Calculate_power_windowed(channel, fc=sR, window_len=1*4/trialSplit, window_step=1*4/trialSplit, t_min=0, t_max=4/trialSplit * x + 4/trialSplit)\n",
    "    \n",
    "\n",
    "#     #m_power , std_power\n",
    "#     #print(data_power.shape)\n",
    "#     return data_power\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(591, 2)\n",
      "(591, 4, 7)\n",
      "(197, 4, 7)\n"
     ]
    }
   ],
   "source": [
    "#Splitting into training and test data\n",
    "#print(labels)\n",
    "\n",
    "\n",
    "order = np.arange(labels.shape[0])\n",
    "np.random.shuffle(order)\n",
    "\n",
    "temp_data = np.zeros(data.shape)\n",
    "temp_labels = np.zeros(labels.shape)\n",
    "\n",
    "for x in range(labels.shape[0]):\n",
    "    i = order[x]\n",
    "    \n",
    "    temp_data[x] = data[i]\n",
    "    temp_labels[x] = labels[i]\n",
    "\n",
    "data = temp_data\n",
    "labels = temp_labels\n",
    "\n",
    "data_train, data_test = np.split(data, indices_or_sections=[int(labels.shape[0]*0.75)],axis=0)\n",
    "labels_train, labels_test = np.split(labels, indices_or_sections=[int(labels.shape[0]*0.75)],axis=0)\n",
    "print(labels_train.shape)\n",
    "print(data_train.shape)\n",
    "print(data_test.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolution part\n",
    "\n",
    "# Do the convs like I wrote in the Note\n",
    "# First need to find out 8 best channels\n",
    "#  Utilities can help average similar channels\n",
    "# Read through article and how they did it better!\n",
    "# convlayer = tf.keras.layers.Conv2D(4,\n",
    "#  kernel_size=[2,1], input_shape= (data_train.shape[0], data_train.shape[1] , data_train.shape[2], 1,), \n",
    "#  padding=\"valid\", strides=[round(data_train.shape[1]),1],\n",
    "#  activation=\"relu\")\n",
    "\n",
    "\n",
    "# print(data_train.shape)\n",
    "# print(data_test.shape)\n",
    "\n",
    "# #data_train = np.swapaxes(data_train, 2, 1)\n",
    "# #data_test = np.swapaxes(data_test, 2, 1)\n",
    "# print(data_train.shape)\n",
    "# print(data_test.shape)\n",
    "# data_train = np.expand_dims(data_train, axis=0)\n",
    "# data_test = np.expand_dims(data_test, axis=0)\n",
    "# data_train = np.moveaxis(data_train, 0, -1)\n",
    "# data_test = np.moveaxis(data_test, 0, -1)\n",
    "\n",
    "# conv_data_train = convlayer(data_train)\n",
    "# conv_data_test = convlayer(data_test)\n",
    "\n",
    "# data_train = conv_data_train\n",
    "# data_test = conv_data_test\n",
    "\n",
    "# #data_train = np.swapaxes(data_train, 2, 1)\n",
    "# #data_test = np.swapaxes(data_test, 2, 1)\n",
    "\n",
    "\n",
    "# data_train = np.squeeze(np.moveaxis(data_train, -1, 0))\n",
    "# data_test = np.squeeze(np.moveaxis(data_test, -1, 0))\n",
    "# print(data_train.shape)\n",
    "# print(data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 28)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 84)                2436      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 784)               66640     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 21952)             17232320  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 21952)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 784)               17211152  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 28)                21980     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 28)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 58        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 34,534,586\n",
      "Trainable params: 34,534,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "eeg_model = tf.keras.Sequential([\n",
    "    \n",
    "    #layers.Conv2D(500,\n",
    "    #kernel_size=[1,8], input_shape= (1, data_train.shape[1], data_train.shape[2], 1,), \n",
    "    #padding=\"valid\", strides=[1,4],\n",
    "    #activation=\"relu\"),\n",
    "    # layers.Conv2D(200,\n",
    "    # kernel_size=[1,32], \n",
    "    # padding=\"valid\", strides=1,\n",
    "    # activation=\"relu\"),\n",
    "    # layers.Conv2D(200,\n",
    "    # kernel_size=[1,32], \n",
    "    # padding=\"valid\", strides=1,\n",
    "    # activation=\"relu\"),\n",
    "    # layers.Conv2D(200,\n",
    "    # kernel_size=[1,32], \n",
    "    # padding=\"valid\", strides=4,\n",
    "    # activation=\"relu\"),\n",
    "    layers.Flatten(input_shape = (data_train.shape[1],data_train.shape[2])),\n",
    "    layers.Dense(units=28*3, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(units=28*28, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(units=28*28*28, activation=\"relu\"),\n",
    "    #layers.Dense(units=20, activation=\"relu\"),\n",
    "    #layers.Dense(units=5, activation=\"relu\"),\n",
    "    #layers.Dense(units=1, activation=\"relu\"),\n",
    "    #layers.Flatten(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(units=28*28, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(units=28, activation=\"relu\"),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(units=2, activation=\"softmax\")\n",
    "\n",
    "\n",
    "])\n",
    "eeg_model.build()\n",
    "eeg_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for trialNr, trial in enumerate(data_train[46:47],1):\n",
    "    for channel in trial[3:4]:\n",
    "        plt.figure()\n",
    "        plt.plot(channel)\n",
    "        plt.title(\"EEG {}\".format(labels_train[trialNr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "19/19 [==============================] - 4s 104ms/step - loss: 0.7019 - accuracy: 0.5245\n",
      "Epoch 2/200\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.6945 - accuracy: 0.5110\n",
      "Epoch 3/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.6923 - accuracy: 0.5212\n",
      "Epoch 4/200\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6936 - accuracy: 0.5161\n",
      "Epoch 5/200\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6943 - accuracy: 0.5347\n",
      "Epoch 6/200\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6941 - accuracy: 0.5127\n",
      "Epoch 7/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.6922 - accuracy: 0.5144\n",
      "Epoch 8/200\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6932 - accuracy: 0.4975\n",
      "Epoch 9/200\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.6914 - accuracy: 0.5313\n",
      "Epoch 10/200\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.6902 - accuracy: 0.5279\n",
      "Epoch 11/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.6842 - accuracy: 0.5668\n",
      "Epoch 12/200\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.6912 - accuracy: 0.5364\n",
      "Epoch 13/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.6905 - accuracy: 0.5093\n",
      "Epoch 14/200\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6882 - accuracy: 0.5398\n",
      "Epoch 15/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.6794 - accuracy: 0.5618\n",
      "Epoch 16/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.6844 - accuracy: 0.5871\n",
      "Epoch 17/200\n",
      "19/19 [==============================] - 2s 100ms/step - loss: 0.6764 - accuracy: 0.5685\n",
      "Epoch 18/200\n",
      "19/19 [==============================] - 2s 107ms/step - loss: 0.6652 - accuracy: 0.5905\n",
      "Epoch 19/200\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.6690 - accuracy: 0.5770\n",
      "Epoch 20/200\n",
      "19/19 [==============================] - 2s 105ms/step - loss: 0.6740 - accuracy: 0.5939\n",
      "Epoch 21/200\n",
      "19/19 [==============================] - 2s 102ms/step - loss: 0.6553 - accuracy: 0.6362\n",
      "Epoch 22/200\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.6701 - accuracy: 0.5838\n",
      "Epoch 23/200\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.6575 - accuracy: 0.6125\n",
      "Epoch 24/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.6487 - accuracy: 0.6227\n",
      "Epoch 25/200\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.6384 - accuracy: 0.6091\n",
      "Epoch 26/200\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.6310 - accuracy: 0.6176\n",
      "Epoch 27/200\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.6150 - accuracy: 0.6497\n",
      "Epoch 28/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.5992 - accuracy: 0.6582\n",
      "Epoch 29/200\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.6295 - accuracy: 0.6464\n",
      "Epoch 30/200\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.6183 - accuracy: 0.6362\n",
      "Epoch 31/200\n",
      "19/19 [==============================] - 2s 99ms/step - loss: 0.5972 - accuracy: 0.6819\n",
      "Epoch 32/200\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.5740 - accuracy: 0.6870\n",
      "Epoch 33/200\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.6058 - accuracy: 0.6565\n",
      "Epoch 34/200\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.5630 - accuracy: 0.6920\n",
      "Epoch 35/200\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.5879 - accuracy: 0.6937\n",
      "Epoch 36/200\n",
      "19/19 [==============================] - 2s 103ms/step - loss: 0.5398 - accuracy: 0.7090\n",
      "Epoch 37/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.5567 - accuracy: 0.6870\n",
      "Epoch 38/200\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.5323 - accuracy: 0.7191\n",
      "Epoch 39/200\n",
      "19/19 [==============================] - 2s 99ms/step - loss: 0.5107 - accuracy: 0.7310\n",
      "Epoch 40/200\n",
      "19/19 [==============================] - 2s 99ms/step - loss: 0.4846 - accuracy: 0.7462\n",
      "Epoch 41/200\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.5118 - accuracy: 0.7208\n",
      "Epoch 42/200\n",
      "19/19 [==============================] - 2s 90ms/step - loss: 0.5090 - accuracy: 0.7394\n",
      "Epoch 43/200\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.4717 - accuracy: 0.7597\n",
      "Epoch 44/200\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.4416 - accuracy: 0.7953\n",
      "Epoch 45/200\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.4679 - accuracy: 0.7563\n",
      "Epoch 46/200\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.4429 - accuracy: 0.7750\n",
      "Epoch 47/200\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.4625 - accuracy: 0.7614\n",
      "Epoch 48/200\n",
      "19/19 [==============================] - 2s 102ms/step - loss: 0.3963 - accuracy: 0.8206\n",
      "Epoch 49/200\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.4011 - accuracy: 0.8105\n",
      "Epoch 50/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.4211 - accuracy: 0.7817\n",
      "Epoch 51/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.4072 - accuracy: 0.7986\n",
      "Epoch 52/200\n",
      "19/19 [==============================] - 2s 100ms/step - loss: 0.3895 - accuracy: 0.8054\n",
      "Epoch 53/200\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.3730 - accuracy: 0.8105\n",
      "Epoch 54/200\n",
      "19/19 [==============================] - 2s 105ms/step - loss: 0.3623 - accuracy: 0.8308\n",
      "Epoch 55/200\n",
      "19/19 [==============================] - 2s 102ms/step - loss: 0.3209 - accuracy: 0.8545\n",
      "Epoch 56/200\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.3587 - accuracy: 0.8359\n",
      "Epoch 57/200\n",
      "19/19 [==============================] - 2s 99ms/step - loss: 0.3535 - accuracy: 0.8376\n",
      "Epoch 58/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.3125 - accuracy: 0.8629\n",
      "Epoch 59/200\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.3694 - accuracy: 0.8223\n",
      "Epoch 60/200\n",
      "19/19 [==============================] - 2s 91ms/step - loss: 0.3715 - accuracy: 0.8139\n",
      "Epoch 61/200\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.3264 - accuracy: 0.8562\n",
      "Epoch 62/200\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.2962 - accuracy: 0.8629\n",
      "Epoch 63/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.2391 - accuracy: 0.8951\n",
      "Epoch 64/200\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.3474 - accuracy: 0.8409\n",
      "Epoch 65/200\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.2811 - accuracy: 0.8663\n",
      "Epoch 66/200\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.2719 - accuracy: 0.8968\n",
      "Epoch 67/200\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.2549 - accuracy: 0.8849\n",
      "Epoch 68/200\n",
      "19/19 [==============================] - 2s 104ms/step - loss: 0.2368 - accuracy: 0.9120\n",
      "Epoch 69/200\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.2624 - accuracy: 0.9222\n",
      "Epoch 70/200\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.3890 - accuracy: 0.8291\n",
      "Epoch 71/200\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.2641 - accuracy: 0.9019\n",
      "Epoch 72/200\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.2848 - accuracy: 0.8849\n",
      "Epoch 73/200\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.2740 - accuracy: 0.8782\n",
      "Epoch 74/200\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.2135 - accuracy: 0.9205\n",
      "Epoch 75/200\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.1757 - accuracy: 0.9323\n",
      "Epoch 76/200\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.2439 - accuracy: 0.9137\n",
      "Epoch 77/200\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.2864 - accuracy: 0.8765\n",
      "Epoch 78/200\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.2407 - accuracy: 0.8934\n",
      "Epoch 79/200\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.1796 - accuracy: 0.9391\n",
      "Epoch 80/200\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.1446 - accuracy: 0.9323\n",
      "Epoch 81/200\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.1975 - accuracy: 0.9120\n",
      "Epoch 82/200\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.1750 - accuracy: 0.9340\n",
      "Epoch 83/200\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.1289 - accuracy: 0.9560\n",
      "Epoch 84/200\n",
      "19/19 [==============================] - 2s 105ms/step - loss: 0.1210 - accuracy: 0.9594\n",
      "Epoch 85/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.2078 - accuracy: 0.9340\n",
      "Epoch 86/200\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.2726 - accuracy: 0.9019\n",
      "Epoch 87/200\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.2380 - accuracy: 0.8951\n",
      "Epoch 88/200\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.1602 - accuracy: 0.9408\n",
      "Epoch 89/200\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.1407 - accuracy: 0.9459\n",
      "Epoch 90/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.2295 - accuracy: 0.9171\n",
      "Epoch 91/200\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.2058 - accuracy: 0.9289\n",
      "Epoch 92/200\n",
      "19/19 [==============================] - 2s 103ms/step - loss: 0.1764 - accuracy: 0.9272\n",
      "Epoch 93/200\n",
      "19/19 [==============================] - 2s 100ms/step - loss: 0.2648 - accuracy: 0.8951\n",
      "Epoch 94/200\n",
      "19/19 [==============================] - 2s 99ms/step - loss: 0.1791 - accuracy: 0.9188\n",
      "Epoch 95/200\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.1259 - accuracy: 0.9577\n",
      "Epoch 96/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.1250 - accuracy: 0.9577\n",
      "Epoch 97/200\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.1864 - accuracy: 0.9255\n",
      "Epoch 98/200\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.1439 - accuracy: 0.9459\n",
      "Epoch 99/200\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.1210 - accuracy: 0.9594\n",
      "Epoch 100/200\n",
      "19/19 [==============================] - 2s 102ms/step - loss: 0.1132 - accuracy: 0.9577\n",
      "Epoch 101/200\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.1327 - accuracy: 0.9526\n",
      "Epoch 102/200\n",
      "19/19 [==============================] - 2s 103ms/step - loss: 0.0994 - accuracy: 0.9645\n",
      "Epoch 103/200\n",
      "19/19 [==============================] - 2s 100ms/step - loss: 0.0916 - accuracy: 0.9712\n",
      "Epoch 104/200\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.1335 - accuracy: 0.9391\n",
      "Epoch 105/200\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.1307 - accuracy: 0.9560\n",
      "Epoch 106/200\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.1225 - accuracy: 0.9577\n",
      "Epoch 107/200\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.1965 - accuracy: 0.9205\n",
      "Epoch 108/200\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.1455 - accuracy: 0.9543\n",
      "Epoch 109/200\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.0714 - accuracy: 0.9763\n",
      "Epoch 110/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.0294 - accuracy: 0.9949\n",
      "Epoch 111/200\n",
      "19/19 [==============================] - 2s 100ms/step - loss: 0.1102 - accuracy: 0.9746\n",
      "Epoch 112/200\n",
      "19/19 [==============================] - 2s 95ms/step - loss: 0.0850 - accuracy: 0.9780\n",
      "Epoch 113/200\n",
      "19/19 [==============================] - 2s 92ms/step - loss: 0.0738 - accuracy: 0.9763\n",
      "Epoch 114/200\n",
      "19/19 [==============================] - 2s 94ms/step - loss: 0.0873 - accuracy: 0.9746\n",
      "Epoch 115/200\n",
      "19/19 [==============================] - 2s 93ms/step - loss: 0.1289 - accuracy: 0.9560\n",
      "Epoch 116/200\n",
      "19/19 [==============================] - 2s 100ms/step - loss: 0.1639 - accuracy: 0.9357\n",
      "Epoch 117/200\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.0713 - accuracy: 0.9729\n",
      "Epoch 118/200\n",
      "19/19 [==============================] - 2s 105ms/step - loss: 0.0580 - accuracy: 0.9797\n",
      "Epoch 119/200\n",
      "19/19 [==============================] - 2s 102ms/step - loss: 0.1105 - accuracy: 0.9526\n",
      "Epoch 120/200\n",
      "19/19 [==============================] - 2s 101ms/step - loss: 0.0866 - accuracy: 0.9645\n",
      "Epoch 121/200\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.0701 - accuracy: 0.9746\n",
      "Epoch 122/200\n",
      "19/19 [==============================] - 2s 107ms/step - loss: 0.0571 - accuracy: 0.9763\n",
      "Epoch 123/200\n",
      "19/19 [==============================] - 2s 97ms/step - loss: 0.0757 - accuracy: 0.9729\n",
      "Epoch 124/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.0856 - accuracy: 0.9712\n",
      "Epoch 125/200\n",
      "19/19 [==============================] - 2s 98ms/step - loss: 0.1575 - accuracy: 0.9425\n",
      "Epoch 126/200\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.0915 - accuracy: 0.9662\n",
      "Epoch 127/200\n",
      "19/19 [==============================] - 2s 104ms/step - loss: 0.0464 - accuracy: 0.9865\n",
      "Epoch 128/200\n",
      "19/19 [==============================] - 2s 102ms/step - loss: 0.0523 - accuracy: 0.9797\n",
      "Epoch 129/200\n",
      "19/19 [==============================] - 2s 96ms/step - loss: 0.0837 - accuracy: 0.9577\n",
      "Epoch 130/200\n",
      "19/19 [==============================] - 2s 100ms/step - loss: 0.1327 - accuracy: 0.9543\n",
      "Results\n",
      "7/7 [==============================] - 0s 17ms/step - loss: 4.6572 - accuracy: 0.4518\n",
      "7/7 [==============================] - 0s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n",
    "callback2 = tf.keras.callbacks.EarlyStopping(monitor='accuracy', patience=20, restore_best_weights=True)\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(histogram_freq=2)\n",
    "\n",
    "eeg_model.compile(optimizer='adam',\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "data_train_send = np.reshape(data_train, [data_train.shape[0], 1 , data_train.shape[1], data_train.shape[2], 1])\n",
    "#data_train_send = np.reshape(data_train, [data_train.shape[0], 1 , data_train.shape[1], data_train.shape[2], 1])\n",
    "\n",
    "data_test_send = np.reshape(data_test, [data_test.shape[0], 1 , data_test.shape[1], data_test.shape[2], 1])\n",
    "#data_test_send = np.reshape(data_test, [data_test.shape[0], 1 , data_test.shape[1], data_test.shape[2], 1])\n",
    "\n",
    "outputs = eeg_model.fit(data_train, labels_train, callbacks=[callback, callback2], epochs=200) #\n",
    "\n",
    "print(\"Results\")\n",
    "eeg_model.evaluate(data_test, labels_test)\n",
    "result = eeg_model.predict(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50392497 0.496075  ]\n",
      " [0.503717   0.49628296]\n",
      " [0.5056479  0.49435207]\n",
      " [0.50919396 0.49080604]\n",
      " [0.50731146 0.49268854]\n",
      " [0.50498635 0.4950137 ]\n",
      " [0.5036401  0.49635985]\n",
      " [0.5060084  0.49399164]\n",
      " [0.5068587  0.49314126]\n",
      " [0.50509113 0.4949089 ]\n",
      " [0.50717336 0.49282658]\n",
      " [0.50448424 0.49551576]\n",
      " [0.5024281  0.4975719 ]\n",
      " [0.5031996  0.4968004 ]\n",
      " [0.50465834 0.4953416 ]\n",
      " [0.5027685  0.49723148]\n",
      " [0.5055086  0.49449146]\n",
      " [0.5056296  0.4943705 ]\n",
      " [0.504976   0.495024  ]\n",
      " [0.50808114 0.49191886]\n",
      " [0.5076574  0.49234262]\n",
      " [0.50504076 0.49495926]\n",
      " [0.5034468  0.49655318]\n",
      " [0.50331867 0.49668133]\n",
      " [0.5110863  0.48891374]\n",
      " [0.5051015  0.4948984 ]\n",
      " [0.5028853  0.49711478]\n",
      " [0.504805   0.49519497]\n",
      " [0.50361717 0.49638283]\n",
      " [0.5032529  0.4967471 ]\n",
      " [0.5080364  0.49196368]\n",
      " [0.5022429  0.4977571 ]\n",
      " [0.5033033  0.49669668]\n",
      " [0.5035366  0.4964634 ]\n",
      " [0.50271493 0.4972851 ]\n",
      " [0.5065903  0.49340963]\n",
      " [0.50525916 0.49474078]\n",
      " [0.5036601  0.49633995]\n",
      " [0.5040924  0.49590757]\n",
      " [0.5061983  0.49380165]\n",
      " [0.50323683 0.4967631 ]\n",
      " [0.5025557  0.49744427]\n",
      " [0.5032014  0.49679857]\n",
      " [0.50306225 0.49693778]\n",
      " [0.50398546 0.49601457]\n",
      " [0.50398964 0.49601033]\n",
      " [0.50446796 0.49553198]\n",
      " [0.50307655 0.49692348]\n",
      " [0.50417304 0.49582693]\n",
      " [0.50324756 0.49675244]\n",
      " [0.50469136 0.4953086 ]\n",
      " [0.5044881  0.49551192]\n",
      " [0.5031416  0.49685845]\n",
      " [0.50363296 0.49636707]\n",
      " [0.50481015 0.4951898 ]\n",
      " [0.504143   0.49585697]\n",
      " [0.50537527 0.49462476]\n",
      " [0.50296915 0.49703088]\n",
      " [0.50621915 0.4937809 ]\n",
      " [0.50662524 0.49337482]\n",
      " [0.50521314 0.49478686]\n",
      " [0.5050049  0.49499515]\n",
      " [0.50432646 0.4956735 ]\n",
      " [0.5053526  0.4946474 ]\n",
      " [0.5030597  0.49694026]\n",
      " [0.50346816 0.49653184]\n",
      " [0.50388926 0.49611077]\n",
      " [0.5029223  0.4970777 ]\n",
      " [0.5065692  0.49343082]\n",
      " [0.5035959  0.49640414]\n",
      " [0.50286996 0.49713   ]\n",
      " [0.5048305  0.49516946]\n",
      " [0.5049675  0.4950325 ]\n",
      " [0.508703   0.49129704]\n",
      " [0.50569373 0.49430636]\n",
      " [0.50535464 0.4946454 ]\n",
      " [0.5034171  0.4965829 ]\n",
      " [0.5044306  0.49556938]\n",
      " [0.5026939  0.49730605]\n",
      " [0.5059896  0.4940104 ]\n",
      " [0.50526047 0.49473956]\n",
      " [0.5061425  0.49385747]\n",
      " [0.50298005 0.4970199 ]\n",
      " [0.5035286  0.49647138]\n",
      " [0.5035575  0.49644247]\n",
      " [0.50608635 0.49391368]\n",
      " [0.5036606  0.49633932]\n",
      " [0.5039627  0.49603727]\n",
      " [0.50387627 0.49612373]\n",
      " [0.50893974 0.49106035]\n",
      " [0.50462496 0.49537504]\n",
      " [0.5032454  0.49675462]\n",
      " [0.5084412  0.4915588 ]\n",
      " [0.50300276 0.49699724]\n",
      " [0.5027196  0.49728042]\n",
      " [0.50428265 0.49571735]\n",
      " [0.5037032  0.49629688]\n",
      " [0.5041158  0.49588424]\n",
      " [0.50242    0.49758005]\n",
      " [0.50449526 0.49550477]\n",
      " [0.5067542  0.49324572]\n",
      " [0.50286144 0.49713853]\n",
      " [0.50447464 0.49552533]\n",
      " [0.50695246 0.49304754]\n",
      " [0.5051645  0.49483547]\n",
      " [0.50413936 0.49586058]\n",
      " [0.5035807  0.49641934]\n",
      " [0.5029009  0.49709916]\n",
      " [0.50288504 0.49711493]\n",
      " [0.5042988  0.49570116]\n",
      " [0.50605613 0.4939438 ]\n",
      " [0.5039475  0.49605253]\n",
      " [0.50396913 0.49603078]\n",
      " [0.5033231  0.49667692]\n",
      " [0.5037077  0.49629232]\n",
      " [0.50303394 0.49696606]\n",
      " [0.50566363 0.4943364 ]\n",
      " [0.50598055 0.49401942]\n",
      " [0.506349   0.49365094]\n",
      " [0.5036027  0.49639726]\n",
      " [0.5057698  0.49423024]\n",
      " [0.50285184 0.49714813]\n",
      " [0.5059486  0.4940513 ]\n",
      " [0.5058059  0.49419412]\n",
      " [0.50559294 0.4944071 ]\n",
      " [0.5037006  0.4962994 ]\n",
      " [0.503202   0.496798  ]\n",
      " [0.5051258  0.49487424]\n",
      " [0.5039885  0.49601156]\n",
      " [0.5048315  0.4951685 ]\n",
      " [0.5044659  0.49553415]\n",
      " [0.50292295 0.49707702]\n",
      " [0.50469697 0.49530295]\n",
      " [0.50342333 0.49657676]\n",
      " [0.50314355 0.49685654]\n",
      " [0.5065238  0.49347624]\n",
      " [0.5047409  0.4952591 ]\n",
      " [0.5082023  0.49179766]\n",
      " [0.5025156  0.49748433]\n",
      " [0.5053486  0.49465135]\n",
      " [0.502996   0.49700394]\n",
      " [0.50481397 0.495186  ]\n",
      " [0.50269246 0.49730754]\n",
      " [0.5056786  0.4943214 ]\n",
      " [0.5039431  0.49605697]\n",
      " [0.50314146 0.49685857]\n",
      " [0.507657   0.49234307]\n",
      " [0.5027195  0.4972805 ]\n",
      " [0.5039615  0.4960384 ]\n",
      " [0.50525755 0.4947425 ]\n",
      " [0.5029927  0.49700728]\n",
      " [0.5040861  0.49591392]\n",
      " [0.502884   0.49711597]\n",
      " [0.5077661  0.49223393]\n",
      " [0.50371295 0.496287  ]\n",
      " [0.5077767  0.4922233 ]\n",
      " [0.50394475 0.4960553 ]\n",
      " [0.50639254 0.49360746]\n",
      " [0.50392693 0.496073  ]\n",
      " [0.50460225 0.49539778]\n",
      " [0.5027703  0.49722967]\n",
      " [0.50349754 0.49650246]\n",
      " [0.51407003 0.48592994]\n",
      " [0.50356454 0.49643543]\n",
      " [0.50747454 0.49252543]\n",
      " [0.50528216 0.49471784]\n",
      " [0.5044377  0.49556226]\n",
      " [0.5029108  0.4970892 ]\n",
      " [0.5032409  0.49675912]\n",
      " [0.51857084 0.4814292 ]\n",
      " [0.5054308  0.49456915]\n",
      " [0.50487334 0.49512663]\n",
      " [0.5041427  0.49585727]\n",
      " [0.5041994  0.49580058]\n",
      " [0.51103854 0.48896146]\n",
      " [0.5049542  0.49504572]\n",
      " [0.50389034 0.49610963]\n",
      " [0.50453407 0.49546596]\n",
      " [0.50325155 0.49674836]\n",
      " [0.5021183  0.49788168]\n",
      " [0.5096886  0.4903114 ]\n",
      " [0.50264823 0.49735177]\n",
      " [0.5090308  0.49096915]\n",
      " [0.5035209  0.49647912]\n",
      " [0.50490934 0.49509063]\n",
      " [0.5045327  0.49546725]\n",
      " [0.5071435  0.49285647]\n",
      " [0.5043434  0.49565667]\n",
      " [0.5043072  0.49569273]\n",
      " [0.50396514 0.49603483]\n",
      " [0.5028361  0.4971638 ]\n",
      " [0.50259215 0.49740782]\n",
      " [0.50915265 0.49084735]\n",
      " [0.50362945 0.49637052]\n",
      " [0.5088769  0.4911231 ]\n",
      " [0.50639325 0.49360678]\n",
      " [0.50324214 0.49675784]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "result2 = []\n",
    "print(result)\n",
    "for res in result:\n",
    "    x = np.maximum(res[0], res[1])\n",
    "    result2.append(int(np.where(res == x)[0]))\n",
    "\n",
    "result2 = np.array(result2)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for trialNr, trial in enumerate(data[44:46],44):\n",
    "    for channel in trial[5:6]:\n",
    "        plt.figure()\n",
    "        plt.plot(channel)\n",
    "        plt.title(\"EEG {}\".format(labels[trialNr]))\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ForMasterBelgium')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "069d677023565e6a5ddf17d1b4984613dfef886b8c4d74c12f77b96d6ea71cdb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
